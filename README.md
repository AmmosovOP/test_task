# test_task
Тестовое задание.


Нужно создать Airflow DAG со следующей последовательностью

1. Выполнить BigQuery скрипт, который создает таблицу с данными (скрипт простой на ваш выбор)

2. Выполнить BigQuery скрипт, который записывает результаты из предыдущей таблицы в таблицу с историческими данными и партициями по дате выполнения, где хранятся исторические результаты выполнения скрипта из пункта 1.

3. Результаты таблицы из пункта 1. выгружать на GCS.

4. Создать пустой файл на GCS.

5. Скопировать пустой файл из GCS на SFTP сервер.

6. (Операция из пункта 5 триггерит выполнение определенного API), нужно подождать выполнение этого API

7. Передать данные из пункта 3 на SFTP сервер.


Уточнения:

- Важно сделать так, чтобы пути, GCP проекты и другие параметры можно было указывать в конфигурационном файле в зависимости от среды.

- Важно именно использование правильных операторов, правильным образом, на настоящую работу проверять мы не будем
